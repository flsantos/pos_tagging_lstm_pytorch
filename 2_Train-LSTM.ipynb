{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções\n",
    "\n",
    "* Rodar o notebook em ordem\n",
    "* Arquivos necessários:\n",
    "    * preprocessed_CETEN_v2.pkl: gerado pelo notebook 1_Data_Prep.ipynb\n",
    "    * glove_s50.txt: Disponível em http://nilc.icmc.usp.br/embeddings\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from collections import Counter\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_CETEN_v2.pkl', 'rb') as input:\n",
    "    phrases = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format('./glove_s50.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide a base em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_train , phrases_test = train_test_split(phrases,test_size=0.2, random_state=42)\n",
    "#phrases_train , phrases_valid = train_test_split(phrases_train,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converte qualquer palavra com contagem menor que 5 para RARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold = 5\n",
    "\n",
    "word_counts = Counter()\n",
    "for phrase in phrases_train:\n",
    "    for word in phrase:\n",
    "        word_counts[word[0]]+=1\n",
    "        \n",
    "word_counts = {word: count for word, count in word_counts.items() if word_counts[word] >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_train_rare = []\n",
    "RARE_WORD = '__RARE__'\n",
    "for phrase in phrases_train:\n",
    "    phrases_train_rare.append([(w[0] if word_counts.get(w[0]) else RARE_WORD,w[1]) for w in phrase])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contagem de palavras na base de treino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE=100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_counter = Counter()\n",
    "for s in phrases_train_rare[:SAMPLE]:\n",
    "    for tk in s:\n",
    "        tag = tk[1]\n",
    "        if tags_counter[tag]:\n",
    "            tags_counter[tag]=tags_counter[tag]+1\n",
    "        else:\n",
    "            tags_counter[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tags = tags_counter.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for s in phrases_train_rare[:SAMPLE]:\n",
    "    for tk in s:\n",
    "        if word_freq.get(tk[0]) == None:\n",
    "            word_freq[tk[0]] = Counter()\n",
    "        word_freq[tk[0]][tk[1]] = word_freq[tk[0]][tk[1]] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição da lista de TAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tags_pad = ['<PAD>']+list(allowed_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tags_to_ix = { tag:i for i,tag in enumerate(allowed_tags_pad)}\n",
    "ix_to_allowed_tags = { i:tag for i,tag in enumerate(allowed_tags_pad)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect0(lst):\n",
    "    return list(map(lambda x: x[0], lst))\n",
    "def collect1(lst):\n",
    "    return list(map(lambda x: x[1], lst))\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição da base de treino e de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_phrases = [(collect0(p),collect1(p)) for p in phrases_train_rare[:SAMPLE] if collect0(p) != ['']]\n",
    "valid_data_phrases = [(collect0(p),collect1(p)) for p in phrases_train_rare[SAMPLE:SAMPLE+10000] if collect0(p) != ['']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição dos dicionários\n",
    "* word_to_ix\n",
    "* tag_to_ix\n",
    "* ix_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases:  99997\n",
      "Number of words:  59709\n",
      "Number of tags:  17\n"
     ]
    }
   ],
   "source": [
    "training_data = np.array(training_data_phrases)\n",
    "valid_data = np.array(valid_data_phrases)[2000:4000]\n",
    "print(\"Number of phrases: \", len(training_data))\n",
    "word_to_ix = {'<PAD>':0}\n",
    "char_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        \n",
    "        for char in word:\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "print(\"Number of words: \", len(word_to_ix))\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "tag_to_ix = allowed_tags_to_ix\n",
    "ix_to_tag = ix_to_allowed_tags\n",
    "print(\"Number of tags: \", len(allowed_tags_to_ix.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição das dimensões da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva dicionários: serão carregados para a etapa de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_lstm_dicts.pkl', 'wb') as f:\n",
    "    pickle.dump({'word_to_ix':word_to_ix, 'tag_to_ix':tag_to_ix, 'ix_to_tag':ix_to_tag, 'BATCH_SIZE':1}, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializa embeddings weights com Glove50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44774\n"
     ]
    }
   ],
   "source": [
    "matrix_len = len(word_to_ix)\n",
    "weights_matrix = np.zeros((matrix_len, 50))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(word_to_ix.keys()):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "print(words_found)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da rede LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1eca3d0d930>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BACKUP WORKING LSTM biredirectional with gloves100\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, word_to_ix, embedding_weights):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tagset_size = tagset_size\n",
    "\n",
    "        padding_idx = word_to_ix['<PAD>']\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(embedding_weights))\n",
    "        \n",
    "        self.num_layers = 1\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, self.num_layers, batch_first=False, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers*2, BATCH_SIZE, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers*2, BATCH_SIZE, self.hidden_dim))\n",
    "        #return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "    def forward(self, sentence, s_lengths, debug=False):\n",
    "        batch_size, seq_len, = sentence.size()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if debug: print(\"sentences input:\", sentence.size())\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len, embedding_dim)\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        if debug: print(\"embeds:\",embeds.size())\n",
    "        \n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "        \n",
    "        embeds = embeds.transpose(0,1)\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        lstm_input = torch.nn.utils.rnn.pack_padded_sequence(embeds, s_lengths, batch_first=False)        \n",
    "        #lstm_input = embeds.view(seq_len, BATCH_SIZE, -1)\n",
    "        #lstm_input = embeds.transpose(0,1)\n",
    "        if debug: print(\"lstm_input:\",lstm_input.data.size())\n",
    "        \n",
    "        if debug: print(\"hidden0 (ht):\",self.hidden[0].size())\n",
    "        if debug: print(\"hidden1 (hc):\",self.hidden[1].size())    \n",
    "        # now run through LSTM\n",
    "        lstm_out, self.hidden = self.lstm(lstm_input, self.hidden)\n",
    "        \n",
    "        if debug: print(\"lstm_out:\", lstm_out.data.size())\n",
    "        if debug: print(\"hidden0 (ht):\",self.hidden[0].size())\n",
    "        if debug: print(\"hidden1 (hc):\",self.hidden[1].size())    \n",
    "        \n",
    "        # undo the packing operation\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=False)\n",
    "        #print(\"lstm_out:\", lstm_out.data.size())\n",
    "        \n",
    "        \n",
    "        # 3. Project to tag space\n",
    "        # Dim transformation: (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        #lstm_out = lstm_out.contiguous()\n",
    "        #lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        #lstm_out = lstm_out.view(seq_len, -1)\n",
    "        \n",
    "        ##OPTION1 \n",
    "        #batch_first = lstm_out.transpose(0,1)\n",
    "        #print(\"batch_first:\", batch_first.size())\n",
    "        #batch_first = batch_first.contiguous()\n",
    "        #print(\"batch_first(contiguous):\", batch_first.size())\n",
    "        #linear_input = batch_first.view(BATCH_SIZE, -1)\n",
    "        ### Para fazer isso, preciso setar o input da Linear ao inves de 200, para 200*N,\n",
    "        #onde N é o tamanho da maior sentença da base toda\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###OPTION2\n",
    "        #linear_input = self.hidden[0]\n",
    "        \n",
    "        \n",
    "        ##option3\n",
    "        lstm_out = lstm_out.transpose(0,1)\n",
    "        lstm_out = lstm_out.contiguous()\n",
    "        if debug: print(\"lstm_out reshaped:\", lstm_out.size())\n",
    "        linear_input = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        if debug: print(\"linear_input:\", linear_input.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        tag_space = self.hidden2tag(linear_input)\n",
    "        if debug: print(\"hidden out :\", tag_space.size())\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        #tag_scores = tag_scores.view(BATCH_SIZE, seq_len, self.tagset_size)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testa se o feed-forwaed está funcionando!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = training_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences input: torch.Size([1, 19])\n",
      "embeds: torch.Size([1, 19, 50])\n",
      "lstm_input: torch.Size([19, 50])\n",
      "hidden0 (ht): torch.Size([2, 1, 200])\n",
      "hidden1 (hc): torch.Size([2, 1, 200])\n",
      "lstm_out: torch.Size([19, 400])\n",
      "hidden0 (ht): torch.Size([2, 1, 200])\n",
      "hidden1 (hc): torch.Size([2, 1, 200])\n",
      "lstm_out reshaped: torch.Size([1, 19, 400])\n",
      "linear_input: torch.Size([19, 400])\n",
      "hidden out : torch.Size([19, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 17])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=1\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix), word_to_ix,weights_matrix)\n",
    "\n",
    "#tag_scores = model(sentence_in, s_lengths)\n",
    "inputs = prepare_batch_sequence([test_sentence]*BATCH_SIZE, word_to_ix)\n",
    "tag_scores = model(inputs, [len(inputs[0])]*BATCH_SIZE, debug=True)\n",
    "tag_scores.shape\n",
    "#tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição de funções auxiliares para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_sequence(sentences, to_ix):\n",
    "    s_lengths = [len(s) for s in sentences]\n",
    "\n",
    "    # create an empty matrix with padding tokens\n",
    "    pad_token = to_ix['<PAD>']\n",
    "    longest_sent = max(s_lengths)\n",
    "    batch_size = len(sentences)\n",
    "    padded_sentences = np.ones((batch_size, longest_sent)) * pad_token\n",
    "    # copy over the actual sequences\n",
    "    for n, s_len in enumerate(s_lengths):\n",
    "        sequence = sentences[n]\n",
    "        idxs = [to_ix[w] for w in sequence[:s_len]]\n",
    "        padded_sentences[n, 0:s_len] = idxs\n",
    "    \n",
    "    return torch.tensor(padded_sentences, dtype=torch.long)\n",
    "\n",
    "def batch_idx_loader(data, shuffle=True):\n",
    "    permutation = torch.randperm(len(data)) if shuffle else torch.tensor(range(len(data)))\n",
    "    gen = (permutation[b:b+BATCH_SIZE] for b in range(0,len(data), BATCH_SIZE))\n",
    "    return gen\n",
    "\n",
    "def get_data_sorted(data, idx):\n",
    "    data = np.array(data)\n",
    "    idx = [idx] if len(idx) == 1 else idx\n",
    "    \n",
    "    sentences = [bat[0] for bat in data[idx]]\n",
    "    tags =      [bat[1] for bat in data[idx]]\n",
    "\n",
    "    mydict     = {idx:len(s) for idx,s in enumerate(sentences)}\n",
    "    idx_sorted = [k for k in sorted(mydict, key=mydict.get, reverse=True)]\n",
    "\n",
    "    sentences = np.array(sentences)[idx_sorted]\n",
    "    tags      = np.array(tags)[idx_sorted]\n",
    "    \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [1,  1000,  1000] loss: 12.400  acc:64.33 //// Valid  loss: 0.910  acc:74.57\n",
      "Train: [1,  2000,  2000] loss: 6.942  acc:79.10 //// Valid  loss: 0.713  acc:81.71\n",
      "Train: [1,  3000,  3000] loss: 5.883  acc:81.83 //// Valid  loss: 0.604  acc:84.27\n",
      "Train: [1,  4000,  4000] loss: 5.102  acc:84.64 //// Valid  loss: 0.572  acc:85.65\n",
      "Train: [1,  5000,  5000] loss: 4.908  acc:85.90 //// Valid  loss: 0.531  acc:86.77\n",
      "Train: [1,  6000,  6000] loss: 4.399  acc:87.35 //// Valid  loss: 0.549  acc:85.78\n",
      "Train: [1,  7000,  7000] loss: 4.024  acc:87.82 //// Valid  loss: 0.503  acc:87.45\n",
      "Train: [1,  8000,  8000] loss: 3.573  acc:89.37 //// Valid  loss: 0.521  acc:87.87\n",
      "Train: [1,  9000,  9000] loss: 3.729  acc:89.06 //// Valid  loss: 0.445  acc:89.43\n",
      "Train: [1, 10000, 10000] loss: 3.340  acc:90.12 //// Valid  loss: 0.456  acc:89.51\n",
      "Train: [1, 11000, 11000] loss: 3.392  acc:91.00 //// Valid  loss: 0.445  acc:89.48\n",
      "Train: [1, 12000, 12000] loss: 3.254  acc:90.64 //// Valid  loss: 0.388  acc:91.03\n",
      "Train: [1, 13000, 13000] loss: 3.273  acc:90.77 //// Valid  loss: 0.470  acc:89.26\n",
      "Train: [1, 14000, 14000] loss: 3.044  acc:91.51 //// Valid  loss: 0.386  acc:91.31\n",
      "Train: [1, 15000, 15000] loss: 2.853  acc:91.99 //// Valid  loss: 0.396  acc:91.27\n",
      "Train: [1, 16000, 16000] loss: 2.751  acc:91.53 //// Valid  loss: 0.367  acc:91.50\n",
      "Train: [1, 17000, 17000] loss: 2.767  acc:91.85 //// Valid  loss: 0.359  acc:91.92\n",
      "Train: [1, 18000, 18000] loss: 2.610  acc:92.38 //// Valid  loss: 0.377  acc:91.96\n",
      "Train: [1, 19000, 19000] loss: 2.423  acc:92.75 //// Valid  loss: 0.341  acc:92.82\n",
      "Train: [1, 20000, 20000] loss: 2.543  acc:92.91 //// Valid  loss: 0.328  acc:92.81\n",
      "Train: [1, 21000, 21000] loss: 2.511  acc:92.90 //// Valid  loss: 0.332  acc:92.91\n",
      "Train: [1, 22000, 22000] loss: 2.629  acc:92.94 //// Valid  loss: 0.356  acc:92.84\n",
      "Train: [1, 23000, 23000] loss: 2.321  acc:93.14 //// Valid  loss: 0.329  acc:92.91\n",
      "Train: [1, 24000, 24000] loss: 2.419  acc:93.44 //// Valid  loss: 0.311  acc:93.46\n",
      "Train: [1, 25000, 25000] loss: 2.313  acc:93.47 //// Valid  loss: 0.330  acc:92.73\n",
      "Train: [1, 26000, 26000] loss: 2.259  acc:93.40 //// Valid  loss: 0.304  acc:93.25\n",
      "Train: [1, 27000, 27000] loss: 2.204  acc:93.39 //// Valid  loss: 0.291  acc:93.60\n",
      "Train: [1, 28000, 28000] loss: 2.382  acc:93.64 //// Valid  loss: 0.312  acc:93.24\n",
      "Train: [1, 29000, 29000] loss: 2.228  acc:93.82 //// Valid  loss: 0.327  acc:93.44\n",
      "Train: [1, 30000, 30000] loss: 2.222  acc:93.98 //// Valid  loss: 0.305  acc:93.57\n",
      "Train: [1, 31000, 31000] loss: 2.352  acc:93.62 //// Valid  loss: 0.298  acc:93.75\n",
      "Train: [1, 32000, 32000] loss: 2.215  acc:93.78 //// Valid  loss: 0.308  acc:93.78\n",
      "Train: [1, 33000, 33000] loss: 2.102  acc:94.12 //// Valid  loss: 0.283  acc:94.04\n",
      "Train: [1, 34000, 34000] loss: 1.934  acc:94.31 //// Valid  loss: 0.299  acc:94.18\n",
      "Train: [1, 35000, 35000] loss: 2.064  acc:94.29 //// Valid  loss: 0.320  acc:93.95\n",
      "Train: [1, 36000, 36000] loss: 2.064  acc:94.16 //// Valid  loss: 0.287  acc:94.31\n",
      "Train: [1, 37000, 37000] loss: 1.936  acc:94.66 //// Valid  loss: 0.271  acc:94.27\n",
      "Train: [1, 38000, 38000] loss: 1.877  acc:94.42 //// Valid  loss: 0.256  acc:94.41\n",
      "Train: [1, 39000, 39000] loss: 2.104  acc:94.45 //// Valid  loss: 0.289  acc:94.19\n",
      "Train: [1, 40000, 40000] loss: 1.957  acc:94.69 //// Valid  loss: 0.300  acc:94.29\n",
      "Train: [1, 41000, 41000] loss: 1.885  acc:94.68 //// Valid  loss: 0.287  acc:94.33\n",
      "Train: [1, 42000, 42000] loss: 1.795  acc:94.93 //// Valid  loss: 0.302  acc:94.34\n",
      "Train: [1, 43000, 43000] loss: 1.823  acc:94.91 //// Valid  loss: 0.262  acc:94.61\n",
      "Train: [1, 44000, 44000] loss: 1.910  acc:94.91 //// Valid  loss: 0.272  acc:94.44\n",
      "Train: [1, 45000, 45000] loss: 1.933  acc:94.67 //// Valid  loss: 0.261  acc:94.73\n",
      "Train: [1, 46000, 46000] loss: 1.936  acc:94.71 //// Valid  loss: 0.288  acc:94.61\n",
      "Train: [1, 47000, 47000] loss: 2.103  acc:94.44 //// Valid  loss: 0.259  acc:94.83\n",
      "Train: [1, 48000, 48000] loss: 1.834  acc:94.98 //// Valid  loss: 0.275  acc:94.42\n",
      "Train: [1, 49000, 49000] loss: 1.810  acc:94.76 //// Valid  loss: 0.286  acc:94.46\n",
      "Train: [1, 50000, 50000] loss: 1.671  acc:95.27 //// Valid  loss: 0.262  acc:94.88\n",
      "Train: [1, 51000, 51000] loss: 1.884  acc:94.89 //// Valid  loss: 0.256  acc:94.78\n",
      "Train: [1, 52000, 52000] loss: 1.907  acc:94.96 //// Valid  loss: 0.267  acc:94.63\n",
      "Train: [1, 53000, 53000] loss: 1.813  acc:95.08 //// Valid  loss: 0.289  acc:94.25\n",
      "Train: [1, 54000, 54000] loss: 1.932  acc:95.13 //// Valid  loss: 0.257  acc:94.78\n",
      "Train: [1, 55000, 55000] loss: 1.806  acc:95.14 //// Valid  loss: 0.262  acc:94.70\n",
      "Train: [1, 56000, 56000] loss: 1.650  acc:95.14 //// Valid  loss: 0.283  acc:94.81\n",
      "Train: [1, 57000, 57000] loss: 1.717  acc:95.08 //// Valid  loss: 0.259  acc:94.79\n",
      "Train: [1, 58000, 58000] loss: 1.807  acc:94.83 //// Valid  loss: 0.249  acc:94.85\n",
      "Train: [1, 59000, 59000] loss: 1.916  acc:95.14 //// Valid  loss: 0.245  acc:95.06\n",
      "Train: [1, 60000, 60000] loss: 1.799  acc:95.09 //// Valid  loss: 0.267  acc:94.80\n",
      "Train: [1, 61000, 61000] loss: 1.743  acc:95.24 //// Valid  loss: 0.252  acc:95.01\n",
      "Train: [1, 62000, 62000] loss: 1.614  acc:95.27 //// Valid  loss: 0.228  acc:95.34\n",
      "Train: [1, 63000, 63000] loss: 1.631  acc:95.22 //// Valid  loss: 0.263  acc:94.73\n",
      "Train: [1, 64000, 64000] loss: 1.580  acc:95.53 //// Valid  loss: 0.234  acc:95.19\n",
      "Train: [1, 65000, 65000] loss: 1.669  acc:95.57 //// Valid  loss: 0.274  acc:94.78\n",
      "Train: [1, 66000, 66000] loss: 1.587  acc:95.20 //// Valid  loss: 0.241  acc:95.16\n",
      "Train: [1, 67000, 67000] loss: 1.732  acc:95.54 //// Valid  loss: 0.247  acc:95.24\n",
      "Train: [1, 68000, 68000] loss: 1.651  acc:95.68 //// Valid  loss: 0.292  acc:95.10\n",
      "Train: [1, 69000, 69000] loss: 1.666  acc:95.40 //// Valid  loss: 0.265  acc:95.09\n",
      "Train: [1, 70000, 70000] loss: 1.615  acc:95.61 //// Valid  loss: 0.249  acc:95.18\n",
      "Train: [1, 71000, 71000] loss: 1.698  acc:95.34 //// Valid  loss: 0.257  acc:95.23\n",
      "Train: [1, 72000, 72000] loss: 1.650  acc:95.63 //// Valid  loss: 0.228  acc:95.30\n",
      "Train: [1, 73000, 73000] loss: 1.595  acc:95.74 //// Valid  loss: 0.238  acc:95.50\n",
      "Train: [1, 74000, 74000] loss: 1.538  acc:95.80 //// Valid  loss: 0.229  acc:95.34\n",
      "Train: [1, 75000, 75000] loss: 1.705  acc:95.39 //// Valid  loss: 0.265  acc:95.24\n",
      "Train: [1, 76000, 76000] loss: 1.652  acc:95.31 //// Valid  loss: 0.246  acc:95.12\n",
      "Train: [1, 77000, 77000] loss: 1.490  acc:95.68 //// Valid  loss: 0.249  acc:95.02\n",
      "Train: [1, 78000, 78000] loss: 1.516  acc:95.63 //// Valid  loss: 0.280  acc:95.28\n",
      "Train: [1, 79000, 79000] loss: 1.664  acc:95.66 //// Valid  loss: 0.255  acc:95.37\n",
      "Train: [1, 80000, 80000] loss: 1.483  acc:95.50 //// Valid  loss: 0.254  acc:95.07\n",
      "Train: [1, 81000, 81000] loss: 1.585  acc:95.51 //// Valid  loss: 0.255  acc:95.42\n",
      "Train: [1, 82000, 82000] loss: 1.461  acc:95.77 //// Valid  loss: 0.265  acc:95.40\n",
      "Train: [1, 83000, 83000] loss: 1.468  acc:95.87 //// Valid  loss: 0.247  acc:95.48\n",
      "Train: [1, 84000, 84000] loss: 1.511  acc:96.01 //// Valid  loss: 0.248  acc:95.38\n",
      "Train: [1, 85000, 85000] loss: 1.567  acc:95.55 //// Valid  loss: 0.252  acc:95.50\n",
      "Train: [1, 86000, 86000] loss: 1.421  acc:95.87 //// Valid  loss: 0.244  acc:95.37\n",
      "Train: [1, 87000, 87000] loss: 1.556  acc:95.64 //// Valid  loss: 0.237  acc:95.36\n",
      "Train: [1, 88000, 88000] loss: 1.393  acc:96.00 //// Valid  loss: 0.237  acc:95.42\n",
      "Train: [1, 89000, 89000] loss: 1.472  acc:95.84 //// Valid  loss: 0.265  acc:95.18\n",
      "Train: [1, 90000, 90000] loss: 1.328  acc:95.86 //// Valid  loss: 0.229  acc:95.55\n",
      "Train: [1, 91000, 91000] loss: 1.495  acc:96.01 //// Valid  loss: 0.247  acc:95.43\n",
      "Train: [1, 92000, 92000] loss: 1.345  acc:96.17 //// Valid  loss: 0.222  acc:95.72\n",
      "Train: [1, 93000, 93000] loss: 1.460  acc:96.15 //// Valid  loss: 0.240  acc:95.56\n",
      "Train: [1, 94000, 94000] loss: 1.466  acc:96.23 //// Valid  loss: 0.231  acc:95.62\n",
      "Train: [1, 95000, 95000] loss: 1.473  acc:95.86 //// Valid  loss: 0.227  acc:95.53\n",
      "Train: [1, 96000, 96000] loss: 1.639  acc:95.73 //// Valid  loss: 0.235  acc:95.60\n",
      "Train: [1, 97000, 97000] loss: 1.473  acc:95.97 //// Valid  loss: 0.232  acc:95.74\n",
      "Train: [1, 98000, 98000] loss: 1.518  acc:95.76 //// Valid  loss: 0.239  acc:95.35\n",
      "Train: [1, 99000, 99000] loss: 1.484  acc:95.98 //// Valid  loss: 0.223  acc:95.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2,  1000,  1000] loss: 1.162  acc:96.52 //// Valid  loss: 0.246  acc:95.67\n",
      "Train: [2,  2000,  2000] loss: 1.421  acc:96.20 //// Valid  loss: 0.220  acc:95.65\n",
      "Train: [2,  3000,  3000] loss: 1.261  acc:96.25 //// Valid  loss: 0.243  acc:95.67\n",
      "Train: [2,  4000,  4000] loss: 1.337  acc:96.23 //// Valid  loss: 0.233  acc:95.66\n",
      "Train: [2,  5000,  5000] loss: 1.193  acc:96.57 //// Valid  loss: 0.220  acc:95.75\n",
      "Train: [2,  6000,  6000] loss: 1.316  acc:96.40 //// Valid  loss: 0.221  acc:95.70\n",
      "Train: [2,  7000,  7000] loss: 1.233  acc:96.39 //// Valid  loss: 0.224  acc:95.79\n",
      "Train: [2,  8000,  8000] loss: 1.375  acc:96.51 //// Valid  loss: 0.229  acc:95.78\n",
      "Train: [2,  9000,  9000] loss: 1.256  acc:96.26 //// Valid  loss: 0.238  acc:95.74\n",
      "Train: [2, 10000, 10000] loss: 1.248  acc:96.21 //// Valid  loss: 0.227  acc:95.32\n",
      "Train: [2, 11000, 11000] loss: 1.123  acc:96.31 //// Valid  loss: 0.233  acc:95.76\n",
      "Train: [2, 12000, 12000] loss: 1.126  acc:96.58 //// Valid  loss: 0.214  acc:95.84\n",
      "Train: [2, 13000, 13000] loss: 1.164  acc:96.37 //// Valid  loss: 0.237  acc:95.79\n",
      "Train: [2, 14000, 14000] loss: 1.307  acc:96.19 //// Valid  loss: 0.222  acc:95.80\n",
      "Train: [2, 15000, 15000] loss: 1.213  acc:96.33 //// Valid  loss: 0.218  acc:96.01\n",
      "Train: [2, 16000, 16000] loss: 1.218  acc:96.42 //// Valid  loss: 0.248  acc:95.68\n",
      "Train: [2, 17000, 17000] loss: 1.210  acc:96.57 //// Valid  loss: 0.256  acc:95.59\n",
      "Train: [2, 18000, 18000] loss: 1.098  acc:96.43 //// Valid  loss: 0.244  acc:95.37\n",
      "Train: [2, 19000, 19000] loss: 1.109  acc:96.49 //// Valid  loss: 0.253  acc:95.87\n",
      "Train: [2, 20000, 20000] loss: 1.234  acc:96.39 //// Valid  loss: 0.253  acc:95.75\n",
      "Train: [2, 21000, 21000] loss: 1.148  acc:96.52 //// Valid  loss: 0.230  acc:96.03\n",
      "Train: [2, 22000, 22000] loss: 1.184  acc:96.29 //// Valid  loss: 0.231  acc:95.97\n",
      "Train: [2, 23000, 23000] loss: 1.150  acc:96.47 //// Valid  loss: 0.225  acc:95.95\n",
      "Train: [2, 24000, 24000] loss: 1.288  acc:96.21 //// Valid  loss: 0.244  acc:95.93\n",
      "Train: [2, 25000, 25000] loss: 1.317  acc:96.46 //// Valid  loss: 0.235  acc:95.98\n",
      "Train: [2, 26000, 26000] loss: 1.163  acc:96.79 //// Valid  loss: 0.234  acc:95.97\n",
      "Train: [2, 27000, 27000] loss: 1.189  acc:96.59 //// Valid  loss: 0.240  acc:95.96\n",
      "Train: [2, 28000, 28000] loss: 1.221  acc:96.74 //// Valid  loss: 0.208  acc:96.02\n",
      "Train: [2, 29000, 29000] loss: 1.006  acc:96.72 //// Valid  loss: 0.221  acc:96.08\n",
      "Train: [2, 30000, 30000] loss: 1.086  acc:96.66 //// Valid  loss: 0.210  acc:96.22\n",
      "Train: [2, 31000, 31000] loss: 1.110  acc:96.52 //// Valid  loss: 0.204  acc:96.23\n",
      "Train: [2, 32000, 32000] loss: 1.282  acc:96.26 //// Valid  loss: 0.261  acc:95.57\n",
      "Train: [2, 33000, 33000] loss: 1.106  acc:96.85 //// Valid  loss: 0.194  acc:96.25\n",
      "Train: [2, 34000, 34000] loss: 1.291  acc:96.44 //// Valid  loss: 0.247  acc:95.88\n",
      "Train: [2, 35000, 35000] loss: 1.088  acc:96.55 //// Valid  loss: 0.207  acc:96.15\n",
      "Train: [2, 36000, 36000] loss: 1.340  acc:96.55 //// Valid  loss: 0.214  acc:96.29\n",
      "Train: [2, 37000, 37000] loss: 1.102  acc:96.68 //// Valid  loss: 0.223  acc:96.14\n",
      "Train: [2, 38000, 38000] loss: 1.080  acc:96.79 //// Valid  loss: 0.214  acc:96.03\n",
      "Train: [2, 39000, 39000] loss: 1.171  acc:96.53 //// Valid  loss: 0.196  acc:96.13\n",
      "Train: [2, 40000, 40000] loss: 1.174  acc:96.64 //// Valid  loss: 0.212  acc:96.13\n",
      "Train: [2, 41000, 41000] loss: 1.049  acc:96.71 //// Valid  loss: 0.212  acc:96.24\n",
      "Train: [2, 42000, 42000] loss: 1.075  acc:96.63 //// Valid  loss: 0.186  acc:96.17\n",
      "Train: [2, 43000, 43000] loss: 1.045  acc:96.52 //// Valid  loss: 0.205  acc:96.25\n",
      "Train: [2, 44000, 44000] loss: 1.276  acc:96.60 //// Valid  loss: 0.226  acc:95.86\n",
      "Train: [2, 45000, 45000] loss: 1.244  acc:96.80 //// Valid  loss: 0.205  acc:95.69\n",
      "Train: [2, 46000, 46000] loss: 1.021  acc:96.86 //// Valid  loss: 0.198  acc:96.35\n",
      "Train: [2, 47000, 47000] loss: 1.187  acc:96.59 //// Valid  loss: 0.234  acc:95.97\n",
      "Train: [2, 48000, 48000] loss: 1.142  acc:96.72 //// Valid  loss: 0.203  acc:96.19\n",
      "Train: [2, 49000, 49000] loss: 1.137  acc:96.75 //// Valid  loss: 0.207  acc:96.25\n",
      "Train: [2, 50000, 50000] loss: 1.175  acc:96.67 //// Valid  loss: 0.239  acc:95.73\n",
      "Train: [2, 51000, 51000] loss: 1.131  acc:96.73 //// Valid  loss: 0.210  acc:96.22\n",
      "Train: [2, 52000, 52000] loss: 1.197  acc:96.54 //// Valid  loss: 0.246  acc:96.01\n",
      "Train: [2, 53000, 53000] loss: 1.145  acc:96.66 //// Valid  loss: 0.201  acc:96.26\n",
      "Train: [2, 54000, 54000] loss: 1.091  acc:96.80 //// Valid  loss: 0.234  acc:95.99\n",
      "Train: [2, 55000, 55000] loss: 1.065  acc:96.70 //// Valid  loss: 0.224  acc:96.07\n",
      "Train: [2, 56000, 56000] loss: 1.184  acc:96.63 //// Valid  loss: 0.232  acc:96.12\n",
      "Train: [2, 57000, 57000] loss: 1.100  acc:96.61 //// Valid  loss: 0.217  acc:96.19\n",
      "Train: [2, 58000, 58000] loss: 1.159  acc:96.66 //// Valid  loss: 0.204  acc:96.30\n",
      "Train: [2, 59000, 59000] loss: 1.212  acc:96.49 //// Valid  loss: 0.219  acc:96.05\n",
      "Train: [2, 60000, 60000] loss: 1.068  acc:96.77 //// Valid  loss: 0.208  acc:96.29\n",
      "Train: [2, 61000, 61000] loss: 1.221  acc:96.60 //// Valid  loss: 0.227  acc:96.06\n",
      "Train: [2, 62000, 62000] loss: 1.130  acc:96.81 //// Valid  loss: 0.198  acc:96.33\n",
      "Train: [2, 63000, 63000] loss: 1.129  acc:96.94 //// Valid  loss: 0.209  acc:96.24\n",
      "Train: [2, 64000, 64000] loss: 0.988  acc:97.13 //// Valid  loss: 0.212  acc:96.25\n",
      "Train: [2, 65000, 65000] loss: 1.103  acc:96.80 //// Valid  loss: 0.203  acc:96.28\n",
      "Train: [2, 66000, 66000] loss: 1.217  acc:96.73 //// Valid  loss: 0.232  acc:96.17\n",
      "Train: [2, 67000, 67000] loss: 1.002  acc:96.82 //// Valid  loss: 0.216  acc:96.32\n",
      "Train: [2, 68000, 68000] loss: 1.120  acc:96.58 //// Valid  loss: 0.193  acc:96.34\n",
      "Train: [2, 69000, 69000] loss: 1.023  acc:96.84 //// Valid  loss: 0.205  acc:96.29\n",
      "Train: [2, 70000, 70000] loss: 1.051  acc:96.80 //// Valid  loss: 0.198  acc:96.33\n",
      "Train: [2, 71000, 71000] loss: 1.159  acc:96.78 //// Valid  loss: 0.200  acc:96.36\n",
      "Train: [2, 72000, 72000] loss: 1.027  acc:96.80 //// Valid  loss: 0.195  acc:96.29\n",
      "Train: [2, 73000, 73000] loss: 1.001  acc:96.94 //// Valid  loss: 0.175  acc:96.38\n",
      "Train: [2, 74000, 74000] loss: 1.051  acc:96.95 //// Valid  loss: 0.236  acc:96.11\n",
      "Train: [2, 75000, 75000] loss: 1.064  acc:96.58 //// Valid  loss: 0.222  acc:96.21\n",
      "Train: [2, 76000, 76000] loss: 1.236  acc:96.35 //// Valid  loss: 0.227  acc:96.12\n",
      "Train: [2, 77000, 77000] loss: 1.035  acc:96.98 //// Valid  loss: 0.194  acc:96.36\n",
      "Train: [2, 78000, 78000] loss: 0.980  acc:96.90 //// Valid  loss: 0.206  acc:96.39\n",
      "Train: [2, 79000, 79000] loss: 1.137  acc:96.78 //// Valid  loss: 0.208  acc:96.32\n",
      "Train: [2, 80000, 80000] loss: 0.962  acc:97.06 //// Valid  loss: 0.209  acc:96.34\n",
      "Train: [2, 81000, 81000] loss: 1.033  acc:96.77 //// Valid  loss: 0.216  acc:96.36\n",
      "Train: [2, 82000, 82000] loss: 0.953  acc:96.93 //// Valid  loss: 0.213  acc:96.29\n",
      "Train: [2, 83000, 83000] loss: 1.103  acc:96.78 //// Valid  loss: 0.196  acc:96.39\n",
      "Train: [2, 84000, 84000] loss: 1.042  acc:97.08 //// Valid  loss: 0.204  acc:96.47\n",
      "Train: [2, 85000, 85000] loss: 0.997  acc:97.06 //// Valid  loss: 0.221  acc:96.30\n",
      "Train: [2, 86000, 86000] loss: 1.160  acc:96.55 //// Valid  loss: 0.251  acc:95.90\n",
      "Train: [2, 87000, 87000] loss: 0.963  acc:97.07 //// Valid  loss: 0.204  acc:96.45\n",
      "Train: [2, 88000, 88000] loss: 1.029  acc:96.92 //// Valid  loss: 0.196  acc:96.41\n",
      "Train: [2, 89000, 89000] loss: 1.130  acc:96.86 //// Valid  loss: 0.200  acc:96.35\n",
      "Train: [2, 90000, 90000] loss: 1.212  acc:96.78 //// Valid  loss: 0.198  acc:96.32\n",
      "Train: [2, 91000, 91000] loss: 1.130  acc:96.72 //// Valid  loss: 0.200  acc:96.30\n",
      "Train: [2, 92000, 92000] loss: 0.945  acc:96.93 //// Valid  loss: 0.195  acc:96.57\n",
      "Train: [2, 93000, 93000] loss: 1.163  acc:96.76 //// Valid  loss: 0.201  acc:96.35\n",
      "Train: [2, 94000, 94000] loss: 0.952  acc:97.11 //// Valid  loss: 0.192  acc:96.46\n",
      "Train: [2, 95000, 95000] loss: 1.067  acc:97.01 //// Valid  loss: 0.220  acc:96.35\n",
      "Train: [2, 96000, 96000] loss: 0.924  acc:97.06 //// Valid  loss: 0.213  acc:96.52\n",
      "Train: [2, 97000, 97000] loss: 1.068  acc:96.65 //// Valid  loss: 0.197  acc:96.45\n",
      "Train: [2, 98000, 98000] loss: 1.048  acc:97.13 //// Valid  loss: 0.178  acc:96.50\n",
      "Train: [2, 99000, 99000] loss: 1.055  acc:96.82 //// Valid  loss: 0.219  acc:96.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [3,  1000,  1000] loss: 0.834  acc:97.18 //// Valid  loss: 0.199  acc:96.34\n",
      "Train: [3,  2000,  2000] loss: 0.882  acc:97.27 //// Valid  loss: 0.197  acc:96.64\n",
      "Train: [3,  3000,  3000] loss: 0.810  acc:97.29 //// Valid  loss: 0.200  acc:96.52\n",
      "Train: [3,  4000,  4000] loss: 0.820  acc:97.38 //// Valid  loss: 0.212  acc:96.47\n",
      "Train: [3,  5000,  5000] loss: 1.000  acc:97.11 //// Valid  loss: 0.197  acc:96.53\n",
      "Train: [3,  6000,  6000] loss: 0.835  acc:97.13 //// Valid  loss: 0.194  acc:96.49\n",
      "Train: [3,  7000,  7000] loss: 0.948  acc:97.06 //// Valid  loss: 0.234  acc:96.19\n",
      "Train: [3,  8000,  8000] loss: 0.815  acc:97.27 //// Valid  loss: 0.186  acc:96.61\n",
      "Train: [3,  9000,  9000] loss: 0.787  acc:97.56 //// Valid  loss: 0.208  acc:96.36\n",
      "Train: [3, 10000, 10000] loss: 0.829  acc:97.20 //// Valid  loss: 0.201  acc:96.49\n",
      "Train: [3, 11000, 11000] loss: 0.749  acc:97.56 //// Valid  loss: 0.196  acc:96.66\n",
      "Train: [3, 12000, 12000] loss: 0.825  acc:97.28 //// Valid  loss: 0.224  acc:96.31\n",
      "Train: [3, 13000, 13000] loss: 0.873  acc:97.23 //// Valid  loss: 0.204  acc:96.60\n",
      "Train: [3, 14000, 14000] loss: 0.962  acc:97.24 //// Valid  loss: 0.245  acc:96.49\n",
      "Train: [3, 15000, 15000] loss: 0.910  acc:97.02 //// Valid  loss: 0.215  acc:96.41\n",
      "Train: [3, 16000, 16000] loss: 0.924  acc:97.28 //// Valid  loss: 0.207  acc:96.52\n",
      "Train: [3, 17000, 17000] loss: 0.865  acc:97.39 //// Valid  loss: 0.198  acc:96.58\n",
      "Train: [3, 18000, 18000] loss: 0.809  acc:97.32 //// Valid  loss: 0.221  acc:96.37\n",
      "Train: [3, 19000, 19000] loss: 0.836  acc:97.26 //// Valid  loss: 0.198  acc:96.48\n",
      "Train: [3, 20000, 20000] loss: 0.842  acc:97.65 //// Valid  loss: 0.205  acc:96.52\n",
      "Train: [3, 21000, 21000] loss: 0.867  acc:97.43 //// Valid  loss: 0.189  acc:96.52\n",
      "Train: [3, 22000, 22000] loss: 0.823  acc:97.44 //// Valid  loss: 0.197  acc:96.69\n",
      "Train: [3, 23000, 23000] loss: 0.801  acc:97.41 //// Valid  loss: 0.195  acc:96.56\n",
      "Train: [3, 24000, 24000] loss: 0.956  acc:97.30 //// Valid  loss: 0.215  acc:96.46\n",
      "Train: [3, 25000, 25000] loss: 0.871  acc:97.32 //// Valid  loss: 0.217  acc:96.48\n",
      "Train: [3, 26000, 26000] loss: 0.925  acc:97.45 //// Valid  loss: 0.194  acc:96.59\n",
      "Train: [3, 27000, 27000] loss: 0.920  acc:97.23 //// Valid  loss: 0.196  acc:96.39\n",
      "Train: [3, 28000, 28000] loss: 0.825  acc:97.51 //// Valid  loss: 0.184  acc:96.68\n",
      "Train: [3, 29000, 29000] loss: 0.861  acc:97.28 //// Valid  loss: 0.215  acc:96.49\n",
      "Train: [3, 30000, 30000] loss: 0.893  acc:97.18 //// Valid  loss: 0.214  acc:96.33\n",
      "Train: [3, 31000, 31000] loss: 0.843  acc:97.54 //// Valid  loss: 0.198  acc:96.56\n",
      "Train: [3, 32000, 32000] loss: 0.932  acc:97.17 //// Valid  loss: 0.185  acc:96.58\n",
      "Train: [3, 33000, 33000] loss: 0.836  acc:97.29 //// Valid  loss: 0.203  acc:96.64\n",
      "Train: [3, 34000, 34000] loss: 0.896  acc:97.32 //// Valid  loss: 0.191  acc:96.50\n",
      "Train: [3, 35000, 35000] loss: 0.813  acc:97.30 //// Valid  loss: 0.196  acc:96.65\n",
      "Train: [3, 36000, 36000] loss: 0.790  acc:97.32 //// Valid  loss: 0.167  acc:96.71\n",
      "Train: [3, 37000, 37000] loss: 0.919  acc:97.42 //// Valid  loss: 0.190  acc:96.58\n",
      "Train: [3, 38000, 38000] loss: 0.804  acc:97.44 //// Valid  loss: 0.199  acc:96.53\n",
      "Train: [3, 39000, 39000] loss: 0.963  acc:97.27 //// Valid  loss: 0.180  acc:96.68\n",
      "Train: [3, 40000, 40000] loss: 0.889  acc:97.29 //// Valid  loss: 0.184  acc:96.58\n",
      "Train: [3, 41000, 41000] loss: 0.834  acc:97.33 //// Valid  loss: 0.199  acc:96.61\n",
      "Train: [3, 42000, 42000] loss: 0.788  acc:97.20 //// Valid  loss: 0.189  acc:96.52\n",
      "Train: [3, 43000, 43000] loss: 0.958  acc:97.19 //// Valid  loss: 0.186  acc:96.74\n",
      "Train: [3, 44000, 44000] loss: 0.880  acc:97.33 //// Valid  loss: 0.190  acc:96.49\n",
      "Train: [3, 45000, 45000] loss: 0.944  acc:97.28 //// Valid  loss: 0.207  acc:96.42\n",
      "Train: [3, 46000, 46000] loss: 0.820  acc:97.28 //// Valid  loss: 0.185  acc:96.70\n",
      "Train: [3, 47000, 47000] loss: 0.745  acc:97.63 //// Valid  loss: 0.201  acc:96.59\n",
      "Train: [3, 48000, 48000] loss: 0.796  acc:97.51 //// Valid  loss: 0.176  acc:96.56\n",
      "Train: [3, 49000, 49000] loss: 0.853  acc:97.18 //// Valid  loss: 0.205  acc:96.62\n",
      "Train: [3, 50000, 50000] loss: 0.857  acc:97.39 //// Valid  loss: 0.178  acc:96.57\n",
      "Train: [3, 51000, 51000] loss: 0.968  acc:97.06 //// Valid  loss: 0.210  acc:96.59\n",
      "Train: [3, 52000, 52000] loss: 0.726  acc:97.54 //// Valid  loss: 0.198  acc:96.69\n",
      "Train: [3, 53000, 53000] loss: 0.810  acc:97.41 //// Valid  loss: 0.190  acc:96.73\n",
      "Train: [3, 54000, 54000] loss: 0.867  acc:97.30 //// Valid  loss: 0.220  acc:96.50\n",
      "Train: [3, 55000, 55000] loss: 0.791  acc:97.36 //// Valid  loss: 0.215  acc:96.41\n",
      "Train: [3, 56000, 56000] loss: 0.881  acc:97.24 //// Valid  loss: 0.217  acc:96.47\n",
      "Train: [3, 57000, 57000] loss: 0.807  acc:97.00 //// Valid  loss: 0.218  acc:96.52\n",
      "Train: [3, 58000, 58000] loss: 0.818  acc:97.35 //// Valid  loss: 0.187  acc:96.68\n",
      "Train: [3, 59000, 59000] loss: 1.010  acc:97.17 //// Valid  loss: 0.220  acc:96.45\n",
      "Train: [3, 60000, 60000] loss: 0.895  acc:97.55 //// Valid  loss: 0.202  acc:96.63\n",
      "Train: [3, 61000, 61000] loss: 0.854  acc:97.31 //// Valid  loss: 0.199  acc:96.70\n",
      "Train: [3, 62000, 62000] loss: 0.835  acc:97.37 //// Valid  loss: 0.203  acc:96.71\n",
      "Train: [3, 63000, 63000] loss: 0.801  acc:97.37 //// Valid  loss: 0.203  acc:96.61\n",
      "Train: [3, 64000, 64000] loss: 0.869  acc:97.46 //// Valid  loss: 0.196  acc:96.69\n",
      "Train: [3, 65000, 65000] loss: 0.779  acc:97.43 //// Valid  loss: 0.199  acc:96.68\n",
      "Train: [3, 66000, 66000] loss: 0.813  acc:97.49 //// Valid  loss: 0.197  acc:96.73\n",
      "Train: [3, 67000, 67000] loss: 0.824  acc:97.45 //// Valid  loss: 0.194  acc:96.76\n",
      "Train: [3, 68000, 68000] loss: 0.907  acc:97.34 //// Valid  loss: 0.175  acc:96.76\n",
      "Train: [3, 69000, 69000] loss: 0.884  acc:97.58 //// Valid  loss: 0.195  acc:96.82\n",
      "Train: [3, 70000, 70000] loss: 0.808  acc:97.42 //// Valid  loss: 0.209  acc:96.63\n",
      "Train: [3, 71000, 71000] loss: 0.866  acc:97.31 //// Valid  loss: 0.192  acc:96.68\n",
      "Train: [3, 72000, 72000] loss: 0.905  acc:97.47 //// Valid  loss: 0.205  acc:96.73\n",
      "Train: [3, 73000, 73000] loss: 0.886  acc:97.42 //// Valid  loss: 0.210  acc:96.72\n",
      "Train: [3, 74000, 74000] loss: 0.782  acc:97.55 //// Valid  loss: 0.179  acc:96.82\n",
      "Train: [3, 75000, 75000] loss: 0.847  acc:97.41 //// Valid  loss: 0.209  acc:96.60\n",
      "Train: [3, 76000, 76000] loss: 0.860  acc:97.32 //// Valid  loss: 0.233  acc:96.45\n",
      "Train: [3, 77000, 77000] loss: 0.834  acc:97.51 //// Valid  loss: 0.182  acc:96.74\n",
      "Train: [3, 78000, 78000] loss: 0.815  acc:97.40 //// Valid  loss: 0.197  acc:96.71\n",
      "Train: [3, 79000, 79000] loss: 0.935  acc:97.31 //// Valid  loss: 0.178  acc:96.75\n",
      "Train: [3, 80000, 80000] loss: 0.801  acc:97.32 //// Valid  loss: 0.200  acc:96.72\n",
      "Train: [3, 81000, 81000] loss: 0.850  acc:97.38 //// Valid  loss: 0.187  acc:96.73\n",
      "Train: [3, 82000, 82000] loss: 0.722  acc:97.53 //// Valid  loss: 0.181  acc:96.72\n",
      "Train: [3, 83000, 83000] loss: 0.826  acc:97.43 //// Valid  loss: 0.179  acc:96.59\n",
      "Train: [3, 84000, 84000] loss: 0.888  acc:97.45 //// Valid  loss: 0.190  acc:96.78\n",
      "Train: [3, 85000, 85000] loss: 0.890  acc:97.43 //// Valid  loss: 0.187  acc:96.68\n",
      "Train: [3, 86000, 86000] loss: 0.755  acc:97.25 //// Valid  loss: 0.183  acc:96.54\n",
      "Train: [3, 87000, 87000] loss: 0.848  acc:97.44 //// Valid  loss: 0.181  acc:96.68\n",
      "Train: [3, 88000, 88000] loss: 0.815  acc:97.18 //// Valid  loss: 0.211  acc:96.63\n",
      "Train: [3, 89000, 89000] loss: 0.957  acc:97.18 //// Valid  loss: 0.182  acc:96.80\n",
      "Train: [3, 90000, 90000] loss: 0.870  acc:97.54 //// Valid  loss: 0.179  acc:96.75\n",
      "Train: [3, 91000, 91000] loss: 0.717  acc:97.54 //// Valid  loss: 0.193  acc:96.64\n",
      "Train: [3, 92000, 92000] loss: 0.783  acc:97.52 //// Valid  loss: 0.180  acc:96.84\n",
      "Train: [3, 93000, 93000] loss: 0.899  acc:97.59 //// Valid  loss: 0.183  acc:96.62\n",
      "Train: [3, 94000, 94000] loss: 0.856  acc:97.51 //// Valid  loss: 0.200  acc:96.72\n",
      "Train: [3, 95000, 95000] loss: 0.766  acc:97.56 //// Valid  loss: 0.187  acc:96.77\n",
      "Train: [3, 96000, 96000] loss: 0.818  acc:97.58 //// Valid  loss: 0.193  acc:96.79\n",
      "Train: [3, 97000, 97000] loss: 0.861  acc:97.42 //// Valid  loss: 0.199  acc:96.89\n",
      "Train: [3, 98000, 98000] loss: 0.897  acc:97.45 //// Valid  loss: 0.222  acc:96.41\n",
      "Train: [3, 99000, 99000] loss: 0.958  acc:97.45 //// Valid  loss: 0.211  acc:96.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (3 epochs)in 59m 11s\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix), word_to_ix, weights_matrix)\n",
    "\n",
    "tag_pad_token = tag_to_ix['<PAD>']\n",
    "loss_function = nn.NLLLoss(ignore_index=tag_pad_token)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "since = time.time()\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE=1\n",
    "for epoch in range(EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_examples = 0\n",
    "    i=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    SAMPLE_C = training_data.shape[0]\n",
    "    SAMPLE_C = int(SAMPLE_C/BATCH_SIZE)*BATCH_SIZE\n",
    "    dt=training_data[:SAMPLE_C]\n",
    "    loader = batch_idx_loader(dt)\n",
    "    for indices in loader:\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        sentences, tags = get_data_sorted(dt, indices)\n",
    "        \n",
    "        i=i+1\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        #sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        sentence_in = prepare_batch_sequence(sentences,word_to_ix)\n",
    "        #chars_in    = prepare_char_sequence(sentences,word_to_ix)\n",
    "        \n",
    "        #targets = prepare_sequence(tags, tag_to_ix)\n",
    "        targets = prepare_batch_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        s_lengths = [len(s) for s in sentences]\n",
    "        tag_scores = model(sentence_in, s_lengths)\n",
    "        _, preds = torch.max(tag_scores, 1)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets.view(-1))\n",
    "        #print(\"vai chamar o backward\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        running_corrects += (preds == targets.view(-1)).cpu().numpy().sum()\n",
    "        running_loss += loss.item()\n",
    "        running_examples += preds.shape[0]\n",
    "        if i % 1000 == 999:    # print every 100 mini-batches\n",
    "            torch.save(model.state_dict(), \"model_ep\"+str(epoch + 1)+\"_bs1_state_dict.model\")\n",
    "            model.eval()\n",
    "            v_running_loss = 0.0\n",
    "            v_running_corrects = 0\n",
    "            v_running_examples = 0\n",
    "            v_i=0\n",
    "            for v_sentence, v_tags in valid_data:\n",
    "                v_i=v_i+1\n",
    "                v_sentence_rare = [w if word_to_ix.get(w) != None else RARE_WORD for w in v_sentence]\n",
    "                v_sentence_in = prepare_batch_sequence([v_sentence_rare]*BATCH_SIZE, word_to_ix)\n",
    "                v_targets = prepare_batch_sequence([v_tags]*BATCH_SIZE, tag_to_ix)\n",
    "                v_seq_len=len(v_sentence_in[0])\n",
    "                v_tag_scores = model(v_sentence_in, [v_seq_len]*BATCH_SIZE)\n",
    "                _, v_preds = torch.max(v_tag_scores, 1)\n",
    "                v_preds = v_preds.view(BATCH_SIZE, v_seq_len)\n",
    "                v_loss = loss_function(v_tag_scores, v_targets.view(-1))\n",
    "                v_running_corrects += (v_preds[0] == v_targets[0]).cpu().numpy().sum()\n",
    "                v_running_loss += v_loss.item()\n",
    "                v_running_examples += v_preds.shape[1]\n",
    "        \n",
    "            #print(v_tag_scores.shape)\n",
    "            #print(v_targets.shape)\n",
    "            #print(v_preds.shape)\n",
    "            #print(v_running_corrects)\n",
    "            #print(v_running_examples)\n",
    "            print('Train: [%d, %5d, %5d] loss: %.4f  acc:%.3f //// Valid  loss: %.4f  acc:%.3f' %\n",
    "                  (epoch + 1,\n",
    "                   i + 1,\n",
    "                   (i+1)*BATCH_SIZE,\n",
    "                   running_loss / 100,\n",
    "                   running_corrects /running_examples*100,\n",
    "                   v_running_loss / valid_data.shape[0],\n",
    "                   v_running_corrects /v_running_examples*100))\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_examples = 0\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete ({} epochs)in {:.0f}m {:.0f}s'.format(\n",
    "    EPOCHS,time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva modelo para ser utilizado na avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_96acc_bs1_state_dict.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LSTMTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model_96acc_bs1_full.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testa performance do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_max(input):\n",
    "    index = 0\n",
    "    for i in range(1, len(input)):\n",
    "        if input[i] > input[index]:\n",
    "            index = i \n",
    "    return index\n",
    "\n",
    "def get_max_prob_result(input, ix_to_tag):\n",
    "    return ix_to_tag[get_index_of_max(input)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa classificação em uma sentença de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ao=longo=de: NUM\n",
      "décadas: N\n",
      "$,: __PUNCT__\n",
      "a: DET\n",
      "limitada: V\n",
      "capacidade: N\n",
      "de: PRP\n",
      "importar: V\n",
      "de: PRP\n",
      "o: DET\n",
      "país: N\n",
      "veio: V\n",
      "preocupando: V\n",
      "os: DET\n",
      "formuladores: N\n",
      "de: PRP\n",
      "política: N\n",
      "econômica: ADJ\n",
      "$.: __PUNCT__\n"
     ]
    }
   ],
   "source": [
    "test_sentence = training_data[0][0]\n",
    "inputs = prepare_batch_sequence([test_sentence]*BATCH_SIZE, word_to_ix)\n",
    "seq_len=len(inputs[0])\n",
    "tag_scores = model(inputs, [seq_len]*BATCH_SIZE)\n",
    "tag_scores = tag_scores.view(BATCH_SIZE, seq_len, len(tag_to_ix))\n",
    "for i in range(len(test_sentence)):\n",
    "    print('{}: {}'.format(test_sentence[i],get_max_prob_result(tag_scores[0][i].data.cpu().numpy(), ix_to_tag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avalia performance em base de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31186, 30520, 32526, 95.88021890180164, 93.83262620672693)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdef=0\n",
    "tloop1=0\n",
    "tloop2=0\n",
    "ttag=0\n",
    "corretas_baseline = 0\n",
    "corretas_rnn = 0\n",
    "totais = 0\n",
    "phrases_test_with_rare = []\n",
    "\n",
    "testwith = 2000\n",
    "testwith = int(testwith/BATCH_SIZE)*BATCH_SIZE\n",
    "\n",
    "### Add RARE \n",
    "for s in phrases_test[:testwith]:\n",
    "    phrases_test_with_rare.append([(tk[0],tk[1]) if word_to_ix.get(tk[0]) != None else (RARE_WORD,tk[1]) for tk in s])\n",
    "    \n",
    "###Define test dataset    \n",
    "validating_data_phrases = [(collect0(p),collect1(p)) for p in phrases_test_with_rare]\n",
    "\n",
    "\n",
    "\n",
    "### Score testdataset into preds variable\n",
    "preds=[]\n",
    "dt = validating_data_phrases\n",
    "loader = batch_idx_loader(dt, shuffle=False)\n",
    "for indices in loader:\n",
    "    sentences, tags = get_data_sorted(dt, indices)\n",
    "    \n",
    "    sentence_in = prepare_batch_sequence(sentences,word_to_ix)\n",
    "    targets = prepare_batch_sequence(tags, tag_to_ix)\n",
    "    \n",
    "    s_lengths = [len(s) for s in sentences]\n",
    "    tag_scores = model(sentence_in, s_lengths)\n",
    "    tag_scores = tag_scores.view(BATCH_SIZE, s_lengths[0], len(tag_to_ix))\n",
    "    \n",
    "    for batchline in range(tag_scores.shape[0]):\n",
    "        pred = [get_max_prob_result(tag_scores[batchline][wordidx].data.cpu().numpy(), ix_to_tag) for wordidx in range(s_lengths[batchline])]\n",
    "        preds.append([(word, golden, tk) for word, golden, tk in zip(sentences[batchline], tags[batchline], pred)])\n",
    "\n",
    "for pred in preds:\n",
    "    for word, tk_golden, tk_pred in pred:\n",
    "        #print(tk_golden)\n",
    "        totais+=1\n",
    "        if tk_golden == word_freq[word].most_common(1)[0][0]:\n",
    "            corretas_baseline+=1\n",
    "        if tk_golden == tk_pred:\n",
    "            corretas_rnn+=1\n",
    "            \n",
    "corretas_rnn, corretas_baseline, totais, corretas_rnn/totais*100, corretas_baseline/totais*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
